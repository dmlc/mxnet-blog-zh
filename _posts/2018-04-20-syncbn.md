---
title: MXNet Gluon上实现跨卡同步Batch Normalization
author: 张航 Amazon AI Applied Scientist
---

很多用户在论坛上，GitHub上，甚至是不同的深度学习平台上，都要求提供跨卡同步Batch Normalization。
我们MXNet团队率先提供了基于Gluon API的实现，并且第一时间提供了一个预览版，希望更快能够得到用户的反馈意见，并且改善API。
在这里我们简要地讲一下Batch Normaliztion的工作方式，以及我们如何实现跨卡同步，还有一些常见的问题回答，
并且欢迎大家到论坛讨论区交流。特别感谢[林海滨](https://github.com/eric-haibin-lin)在后台实现的大力帮助。

**[预览版代码链接](https://github.com/zhanghang1989/MXNet-Gluon-SyncBN)**

### Batch Normalization如何工作，以及它的标准的实现方式

既然是技术贴，读者很多是深学大牛，为什么还要在这里赘述BatchNorm这个简单概念吗？其实不然，很多做科研的朋友如果没有解决过相关问题，
很容易混淆BN在训练和测试时候的工作方式。记得在17年CVPR的[tutoial](http://deeplearning.csail.mit.edu/)上，
何凯明和RBG两位大神分别在自己的talk上都特意对强调了BN的工作原理，可见就算台下都是CV的学者，都有必要复习一遍这些知识。

- 工作原理：

  BN有效地加速了模型训练，加大learning rate，让模型不再过度依赖初始化。它在训练时在网络内部进行归一化(normalization)，
  为训练提供了有效的regularization，抑制过拟合，用原作者的话是防止了协方差偏移。这里上一张图来展示训练模式的BN：

  ![](http://hangzh.com/images/bn1.png)

  其中输入样本$$x={x_0,...x_N}$$，其均值为$$\mu=\sum x_i$$，方差为$$\sigama^2=\frac{\sum (x-\mu)^2}{N}$$，
  BN的输出$$y=\gamma\frac{x-\mu}{\sigma}+\beta$$，$$\gamma\text{与}\beta$$是可学习对参数。
  个人认为，这种强大的效果其实来自于back-propagation时候，来自于**均值和方差对输入样本的梯度**($$\frac(d_\mu}{d_{x_i}} 
  \text{与} \frac{d_\sigma}{d_{x_i}}$$)。这也是BN在训练模式与其在测试模式的重要区别，在测试模式(evaluation mode)下，
  使用训练集上累积的均值和方差，在back-propagation的时候他们对输入样本没有梯度(gradient)。

- 数据并行：

![](http://hangzh.com/images/bn2.png)

### 为什么要跨卡同步 Batch Normalization



### 如何实现

![](http://hangzh.com/images/bn3.png)


我们在最近的论文[Context Encoding for Semantic Segmentation](https://arxiv.org/pdf/1803.08904.pdf)
里面也分享了这种同步一次的方法。

### 常见问答

- 模型是否可以 Hybridize？
  
  训练不行，测试可以。
  MXNet相对于其他平台的显著优势就是提供两套API接口，Symbol API提供静态图速度快，NDArray／Gluon API是impreative执行，
  接口简单好用，而且可以通过hybridize加速训练，这样无缝连接了两套接口。
  目前跨卡BN只提供Gluon接口，在训练时候不能hbridize，
  不过在训练完成之后，BatchNorm在inference的时候不需要跨卡，可以转成普通BN来hybridize。

- 能否使用Symbol API？
  
  目前不行。我们后台增加的operators都是支持Symbol和NDArray两个接口的，所以在构建图的时候完成跨卡操作在理论上是可行的。
  因为笔者是从Gluon API之后开始学习MXNet的，所以目前没有提供Symbol的解决方案，欢迎大家踊跃contribute。

- 训练是否会变慢，能否分布式训练？
  
  变慢是相对的，目前不能分布式。
  相同训练iterations的情况下，训练时间会变长，因为同步的锅(overhead)，但是我们往往可以加大learning rate，
  因为有了跨卡BN，梯度更加平缓了。
  目前已经有相关论文里面说到实现了128卡的分布式训练，证明是可行的，根据竟然这个跨卡BN的latency主要来自于同步，更接近于一个常数，
  所以在大运算量，大网络面前，相对的overhead变得很小。目前我们这个版本还不支持，但是后面可能会做。

### [讨论点这里](https://discuss.gluon.ai/t/topic/1156)
