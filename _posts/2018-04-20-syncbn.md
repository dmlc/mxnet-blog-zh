---
title: MXNet Gluon上实现跨卡同步Batch Normalization
author: 张航 Amazon AI Applied Scientist
---

很多用户在论坛上，GitHub上，甚至是不同的深度学习平台上，都要求提供跨卡同步Batch Normalization。
我们MXNet团队率先提供了基于Gluon API的实现，并且第一时间提供了一个预览版，希望更快能够得到用户的反馈意见，并且改善API。
在这里我们简要地讲一下Batch Normaliztion的工作方式，以及我们如何实现跨卡同步，还有一些常见的问题回答，
并且欢迎大家到论坛讨论区交流。特别感谢[林海滨](https://github.com/eric-haibin-lin)在后台实现的大力帮助。

**[预览版代码链接](https://github.com/zhanghang1989/MXNet-Gluon-SyncBN)**

### Batch Normalization如何工作，以及它的标准的实现方式

可能大家会吐槽，既然是技术贴，读者都是深学大牛，还要在这里赘述BatchNorm这个简单概念吗？其实不然，很多做科研的朋友还是
很容易混淆BN在训练和测试时候的工作方式。
17年CVPR的[tutoial](http://deeplearning.csail.mit.edu/)上，何凯明和RBG两位老师分别在自己的talk上科普了一遍BN。

### 为什么要跨卡同步 Batch Normalization

### 如何实现

### 常见问答

- 能否试用Symbol API？


- 训练是否会变慢，能否分布式训练？


- 模型是否可以 Hybridize？
  
  MXNet相对于其他平台的显著优势就是提供两套API接口，Symbol API提供静态图速度快，NDArray／Gluon API是impreative执行，
  接口简单好用，而且可以通过hybridize加速训练，这样无缝连接了两套接口。目前跨卡BN只提供Gluon接口，在训练时候不能hbridize，
  不过在训练完成之后，BatchNorm在inference的时候不需要跨卡，可以转成普通BN来hybridize。

### [讨论点这里](https://discuss.gluon.ai/t/topic/1156)
