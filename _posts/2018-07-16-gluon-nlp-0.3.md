---
title: GluonNLP 0.3.1 新功能及重现报告
author: 查晟 Amazon Applied Scientist
---

![](img/gluon-nlp-0.3.jpg){:width="500px"}

<span style="color:grey">*谨以此文纪念为 GluonNLP 的模型重现赴汤蹈火，积极踩坑的小哥们。*</span>

[GluonNLP 第一弹](https://mp.weixin.qq.com/s/388JthOSTqbNK3dujWcDCw)中炼丹师小A的遭遇和深度学习那些坑扎了许多朋友的心，比如知乎网友RickZ评论称 "正在被中文数据预处理教做人🙃"。在第一弹发布后，我们 GluonNLP 的小哥们马上又投入到紧张的论文复 (*cai*) 现 (*keng*) 工作中。今天，我们新发布的 GluonNLP 0.3.1 为大家带来了又一批新鲜出炉的模型:
- 语言模型
    - [Cache Language Model](https://gluon-nlp.mxnet.io/api/model.train.html#gluonnlp.model.train.CacheCell) by *Grave, E., et al. “Improving neural language models with a continuous cache”. ICLR 2017*
- 机器翻译
    - [Transformer Model](https://gluon-nlp.mxnet.io/api/scripts/index.html#machine-translation) by *Vaswani, Ashish, et al. "Attention is all you need". NIPS 2017*
- 词向量训练
    - [FastText models](http://gluon-nlp.mxnet.io/api/model.train.html#gluonnlp.model.train.FasttextEmbeddingModel) by *Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). "Enriching Word Vectors with Subword Information". TACL, 5, 135-146.*
    - [Word2Vec](http://gluon-nlp.mxnet.io/api/model.train.html#gluonnlp.model.train.SimpleEmbeddingModel) by *Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). "Distributed representations of words and phrases and their compositionality". NIPS 2014*

开发过程中，各位 NLP 小哥坚持踏踏实实踩坑，在哪个坑跌倒就从哪个坑爬起来，终于攒出大招发了这一波模型。下面我们就来听听各位 GluonNLP 小哥们从前方发来干货满满的踩坑报道。

## 语言模型篇

踩坑小哥: 晨光 [@cgraywang](https://github.com/cgraywang) ([@home](https://sites.google.com/site/raychenguangwang))

![](img/kaibuliaokou.png){:width="300px"}

当时年幼无知的你看到杰伦唱:“*<span style="color:gray">就是开不了口，让她知道</span>*”，心里的潜台词一定是这样的：

![](img/gewhat.jpg){:width="300px"}

*<span style="color:blue">杰伦究竟为啥开不了口？</span>*不行我来，我们感到很生气（暴露了年龄orz）。现在做了GluonNLP的我终于明白，原来是杰伦的*语言模型*不行。如果您也有*类似症状*😀新版本GluonNLP V0.3提供了最先进的治疗方案，即目前市面上表现最佳的*Cache语言模型*，了解一下。

这次新发布的Cache语言模型为啥用过的都说好？因为效果好，能把perplexity (PPL) 从之前最优的 *<span style="color:orange">69.74 提升到 54.51</span>*（PPL是公认的语言模型评价指标，越低越好）。本质上Cache语言模型是*记性更好的深度循环神经网络*。要进一步解释Cache语言模型原理，我们就得言归正传一下，首先传统语言模型的正式定义是：给定一系列词，预测接下来的一个词。传统的语言模型是n元语言模型，简单的来说，就是计算给定n-1个词，预测第n个词的概率（没错~就是条件概率），然后把一个句子中所有的n-gram概率相乘，就是整个句子或者语言的概率。例如二元语言模型(bigram)，就是给定1个词，预测第2个词的概率，然后就能计算出某个句子的语言概率。Cache语言模型的基本假设是：如果一个词已经在一篇文章里面出现过了，那么这个词相对于其他词就更有可能出现在同一篇文章中。例如，*老虎*这个词在维基百科上*老虎*标题的页面中出现的频率是2.8%，而在整个维基百科文章中出现的频率则是0.0037%。Cache语言模型就是通过对语言中存在的词之间的长关联进行更好的建模，从而提升语言模型的效果。换句话说，Cache语言模型有一个叫做Cache的记忆单元，其中包含近期出现过的词（比如同一篇文章，或者某种宽度的窗口中）。简单从模型的实现细节上来讲，Cache存储了长短期记忆（LSTM）模型某段的隐状态（hidden states）与已经看到的词，然后将这段隐状态作为查询关键词，与当前隐状态进行简单的向量之间的内积运算，得到已经看到的词在Cache中的概率分布。该概率分布将直接作用到传统语言模型的概率分布上（例如上面提到的二元语言模型），从而提升语言模型的性能。Cache语言模型主要有三个好处：

- 可以帮助把已经在某个领域的数据上训练好的语言模型直接应用到新的领域；
- 能够更好的预测词典之外的词（OOV），因为只要是见到一次词典之外的词，Cache就能记住；
- 对于语言中的长关联现象有更好的把握。

不过这里要特别说明一下，因为 Cache 语言模型需要把真实的近期出现过的词存储到 Cache 中，所以无法很直接的扩展到生成类的任务（例如，机器翻译或问答系统）。这是因为生成类任务中往往生成时所依赖的前文也是模型自己生成的，而非真实数据。若我们假设过去出现的真实的词是能够很容易拿到的，那么Cache语言模型也就能够直接用到生成类的任务中。对于拿不到真实上文的模型，GluonNLP 中已经有了其他各种语言模型，例如基于循环神经网络（RNN）的语言模型，以及功能强大的AWD语言模型。在这里做个预告，我们将会在后续的博客中针对语言模型跟大家进行更细致全面的分享~

于是自从有了GluonNLP语言模型后的杰伦，不再开不了口，而是变身诗人（*LAOSIJI*）：

![](img/qinghuaci.jpg){:width="300px"}

## 机器翻译篇

踩坑小哥: 帅哥 [@szhengac](https://github.com/szhengac) ([@home](http://www.cse.ust.hk/~szhengac/))

在新的版本里，我们加入了Transformer模型，目前我们的NMT系统支持WMT14和16的英文到德文的互译。Transformer不同于RNN模型，由于整个模型由Attention构成，所有的timestamp的计算可以同时进行，这个大大的加速了训练速度。

在训练Transformer的过程中我们踩了很多的坑。首先是数据的不同， WMT14测试数据newstest2014 英文到德文的翻译有2个版本，一个有3003对样本，另一个有2737对样本；Batch size不再指的是Sentence的个数而是token的个数；再接着是BLEU的计算，这个比较复杂，我们在后面细说。

目前Gluonnlp支持三种BLEU计算：WMT官方的BLEU计算；基于International Tokenization的BLEU；用于重复论文结果的BLEU计算。大家看到这里可能惊讶论文里的BLEU计算方式和官方的算法不一样。这个是我们踩了很久的一个坑。BLEU的计算会由于预处理和tokenization的不同结果千变万化，BLEU的不同可能会有2到3个值。而这个不同其实并不能反应翻译的好坏，因为不同来自于标点符号的处理不同。这大大加了重复论文结果的难度，因为我们可能不知道这篇论文是怎么进行处理的。经过和T2T和Sockeye的作者沟通以及查阅他们的代码，我们才了解到论文里是怎么计算BLEU的。按照论文里的计算方式，我们的Base Transformer模型在newstest2014数据上英文到德文的翻译可以达到28.74的BLEU。这个已经远远超过了论文里报告的Large Transformer的27.3的BLEU，甚至超过了Large Transformer 的28.4的BLEU。而如果按照WMT官方的BLEU计算方式，BLEU的值是26.81。加上International Tokenization的结果是27.65。这里可以看到结果的变化是巨大的。


## 词向量篇

踩坑小哥: Leo [@leezu](https://github.com/leezu) ([@home](http://leonard.lausen.nl/))




## 项目在哪里

最新的 *GluonNLP* 发布在 [gluon-nlp.mxnet.io](https://gluon-nlp.mxnet.io/)。

我们会在接下来的版本里不断加入[新的特性和模型](https://github.com/dmlc/gluon-nlp/releases/latest)。如果对哪些模型特别感兴趣，请戳文末链接给我们留言。

## [讨论请点这里](https://discuss.gluon.ai/t/topic/6330)
