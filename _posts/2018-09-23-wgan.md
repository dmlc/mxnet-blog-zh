<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
---
title: gluon-cv复现wgan
author: 陈晓辰 Tencent Applied Researcher
---

## 简介
本篇博客介绍了如何在gluon-cv中复现[Wasserstein Gan](https://arxiv.org/abs/1701.07875)的，包含了Wgan的一些实现细节和我在使用gluon-cv的过程中踩到的一些坑。

## wgan介绍
Wgan是在Dcgan的基础上，使用wasserstein距离替代KL距离。
![](img/wgan-equation1.svg){:width="500px"}
使用wasserstein距离的好处是，即使当两个概率分布是没有重合的时候，同样能衡量出一个距离，而这种情况下，dcgan中使用的KL距离是会为0，导致梯度消失，训练过程不稳定。
而如果要让discremator能够衡量出wasserstein距离，需要让discremator满足Leibniz连续。
$$ | f(x_{1}) - f(x_{2}) | <= K(x_{1} - x_{2}) $$
具体到神经网络当中的话，就是需要让权重都小于一个比较小的数。

## 关键细节

### Weight clip
对于discremator的权重参数，每个iter前都需要clip到-0.01到0.01的范围
```python
for p in netD.collect_params():
    param = netD.collect_params(p)[p]
    param.set_data(mx.nd.clip(param.data(), opt.clamp_lower, opt.clamp_upper))
```

### weight的初始化
对于generator和discremator都需要初始化权重
```python
for layer in layers:
        classname = layer.__class__.__name__
        if classname.find('Conv') != -1:
            layer.weight.set_data(mx.ndarray.random.normal(0.0,0.02,shape=layer.weight.data().shape))
        elif classname.find('BatchNorm') != -1:
            layer.gamma.set_data(mx.ndarray.random.normal(1.0, 0.02,shape=layer.gamma.data().shape))
            layer.beta.set_data(mx.ndarray.zeros(shape=layer.beta.data().shape))
```

### 关于generator和discremator的迭代次数
算法在每个epoch都会遍历完所有的数据batch，交替训练discremator和generator，在第一个epoch的前面25轮交替训练中，discremator用了100个batch更新100次，generator只用1个batch更新一次，之后的训练中，discremator更新5次，generator更新一次。

### 训练discremator