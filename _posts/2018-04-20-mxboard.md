+---
+title: MXBoard: 助力 MXNet 数据可视化
+author: <a href="https://github.com/reminisce">吴俊</a> Amazon AI Software Engineer
+---

## 写在前面

深度神经网络自出现以来就一直饱受争议。从实践角度来讲，设计并训练出一个可用的模型非常困难，需要涉及大量的调参、修改网络结构、尝试各种优化算法等等；从理论角度来看，深度神经网络的数学理论证明并不完备，从而造成人们对其基本原理缺乏清晰的认识。而数据可视化带来的直观效果可以很大程度上弥补上述的不足。比如，模型训练时如果能实时画出梯度数据分布，可以很快发现并纠正梯度消失或者梯度爆炸的现象。再比如，词嵌入（word embedding）的可视化表明文字根据语义在低维空间聚合成不同的流形（manifold），这也从数学上解释了，为什么增加词向量的维度可以更好地区分不同语义的聚类（低维空间缠绕的流形在高维可以分开）。优秀可视化工具对深度学习的份量可见一斑。

[TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) 的横空出世给 [TensorFlow](https://www.tensorflow.org/) 的使用者带来了可视化的福音。我们曾听到过很多用户（包括企业用户）介绍，选择TensorFlow 是因为可以用 TensorBoard。这么好的东西能否给各大深度学习框架共享呢？多亏了[TeamHG-Memex](https://github.com/TeamHG-Memex/tensorboard_logger) 这个组织，将 TensorFlow 中写数据到事件文件 （event files）的算法提取出来。自此，开发者们只需要将这个算法嵌入到深度学习的框架中，就可以使用 TensorBoard 来可视化框架特有的数据结构了。

正是基于这样的基础，我们开发了一个记录 MXNet 数据类型的库，称作 [MXBoard](https://github.com/awslabs/mxboard)，记录结果得以用 TensorBoard 来呈现。安装 MXBoard 请参考[这里](https://github.com/awslabs/mxboard#installation)。

**注：请安装 MXNet 1.2.0 版本来使用 MXBoard 的全部功能。在 MXNet 1.2.0 正式发布前，请安装 MXNet nightly 版本来使用 MXBoard，`pip install --pre mxnet`**

## MXBoard 快速上手指南

MXBoard 支持了 TensorBoard 中大部分的数据类型，如下图所示。

![](https://github.com/dmlc/web-data/blob/master/mxnet/tensorboard/mxboard_cover.png)

MXBoard API 的设计参考了 [tensorboard-pytorch](https://github.com/lanpa/tensorboard-pytorch)，所有的记录 API 都定义在一个叫 `SummaryWriter` 的类当中，这个类含有诸如记录的文件地址、写文件的频率、写文件的队列大小等等信息，用户可以根据需求设置。当需要把当前数据记录成 TensorBoard 中某种数据类型时，用户只要调用相应的 API 即可。

比如，我们想画一个正态分布标准差逐渐缩小的数据分布图。首先定义一个写记录的对象如下，它会把数据写入到当前文件夹下的名为 `logs` 的文件夹。

```python
from mxboard import 
sw = SummaryWriter(logdir='./logs')
```

接着在每个循环里，用 MXNet 的随机正态分布算子创建一个 `NDArray`，把这个 `NDArray` 传给写数据的 API `add_histogram`，指定画分布图时 `bin` 的数量和当前的循环数。最后，和Python里常用的文件写入器一样，记得关闭这个 `SummaryWriter`。

```python
import mxnet as mx
for i in range(10):
    # create a normal distribution with fixed mean and decreasing std
    data = mx.nd.random.normal(loc=0, scale=10.0/(i+1), shape=(10, 3, 8, 8))
    sw.add_histogram(tag='norml_dist', values=data, bins=200, global_step=i)
sw.close()
```

为了看到效果图，打开命令行窗口，进入到当前文件夹，键入如下命令以打开 TensorBoard：

```bash
tensorboard --logdir=./logs --host=127.0.0.1 --port=8888
```

接着在浏览器地址栏输入 `127.0.0.1:8888`，点击 `HISTOGRAM`，就可以看到如下效果图了。

![](https://github.com/dmlc/web-data/blob/master/mxnet/tensorboard/doc/summary_histogram_norm.png)

## 实战 MXBoard

有了使用 MXBoard 的基本概念，我们来尝试通过可视化完成下面两个任务：

1. 监督模型训练
2. 理解卷积神经网络的工作原理

### 训练 MNIST 模型

借用 Gluon 里训练 [MNIST](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py) 模型的 Python 程序，用 MXBoard 记录下交叉熵、训练和测试精度、参数的梯度数据分布，可以实时反映出模型训练的进度。

首先定义一个 `SummaryWriter` 的对象，

```python
sw = SummaryWriter(logdir='./logs', flush_secs=5)
```

这里加了 `flush_secs=5` 是为了每五秒就写一次记录到文件，以便在浏览器中及时看到结果。接着在每个 mini-batch 循环结束时记录下交叉熵，

```python
sw.add_scalar(tag='cross_entropy', value=L.mean().asscalar(), global_step=global_step)
```

在每个 epoch 结束时记录下参数的梯度为 HISTOGRAM，记录下训练和测试精度为 SCALAR，

```python
grads = [i.grad() for i in net.collect_params().values()]
assert len(grads) == len(param_names)
# logging the gradients of parameters for checking convergence
for i, name in enumerate(param_names):
    sw.add_histogram(tag=name, values=grads[i], global_step=epoch, bins=1000)

name, acc = metric.get()
# logging training accuracy
sw.add_scalar(tag='train_acc', value=acc, global_step=epoch)

name, val_acc = test(ctx)
# logging the validation accuracy
sw.add_scalar(tag='valid_acc', value=val_acc, global_step=epoch)
```

然后运行 Python 程序，并运行 TensorBoard，就可以在浏览器中看到以下效果了。小伙伴们可以尝试着用 MXBoard 监督训练更复杂神经网络。更多本实例的代码和解说请点击[这里](https://github.com/reminisce/mxboard-demo#monitoring-training-mnist-model)。

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/mnist_params_histograms.png)

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/mnist_loss_train_valid_curves.png)

### 可视化卷积层的 filters 和 feature maps

将卷积层的 filters 和 feature maps 当成图片可视化有两个意义：

1. 特征平滑规律的 filters 是模型训练良好的标志之一，未收敛或过拟合模型的卷积层 filters 会出现很多 noise。
2. 观察 filters 和 feature maps 的图片，特别是第一层卷积的图片可以总结出该层所关注的图片特征，这有助于我们理解卷积神经网络的工作原理。

这里将 [MXNet Model Zoo](https://mxnet.incubator.apache.org/model_zoo/index.html) 中三个 CNN 模型，[Inception-BN](http://data.mxnet.io/models/imagenet/inception-bn/)，[Resnet-152](http://data.mxnet.io/models/imagenet/resnet/152-layers/)，和 [VGG16](http://data.mxnet.io/models/imagenet/vgg/) 的 filters 当成图像输出到 TensorBoard，并将这三组 filters 作用于一张黑天鹅的图片（来自[验证数据集 val_256_q90.rec](http://data.mxnet.io/data)）上观察 feature maps。

- Inception-BN

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/inception_bn_conv_1_weight_output.png)

- Resnet-152

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/resnet_152_conv0_weight_output.png)

- VGG16

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/vgg16_conv1_1_weight_output.png)

可以看出三个模型的 filters 都表现出良好的光滑性和规律性，彩色 filters 负责提取原始图片前景和背景的局部特征，灰白图片负责提取图片中物体的轮廓特征。复现代码和解释请点击[这里](https://github.com/reminisce/mxboard-demo#visualizing-filters-of-convnets)。

### 可视化图片的 embedding

最后这个例子比较有趣。Embedding 在自然语言处理中是一个常用的概念，它是真实世界中物体在高维向量空间中的表示。我们也可以借用此概念到卷积神经网络中。卷积神经网络最后一个全连接层的输出可以看成是一个 `batch_size` 行、`num_labels` 列的矩阵，每一行作为一个 `num_labels` 维的向量就是对应输入图片的 embedding。本质上这个 embedding 就是卷积神经网络对图片的编码，`softmax` 层通过此编码来判断图片所属类别。当理解了图片 embedding 的概念后，我们就可以把一个图片集的所有 embedding 通过没有 `softmax` 层的卷积神经网络求出来，调用 MXBoard 的 `add_embedding` API，从而来观察他们在二维或者三维空间中的聚合效应，即同类别图片应该聚合在一起。

这里我们从上一个例子里的验证数据集中随机选取了2304张图片，用 Resnet-152 模型算出了它们的 embeddings，用 MXBoard 写入事件文件，并由 TensorBoard 读取，效果如下。

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/embedding_motion.gif)

这里2304张图片的 embeddings 默认由 [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) 算法压缩到了三维空间，不过图片聚合效应似乎不是那么明显，这是因为 PCA 算法不能保持原始物体之间的空间关系。因此，我们选用 TensorBoard 界面上提供的 [t-SNE](https://lvdmaaten.github.io/tsne/) 算法，重新对 embeddings 进行降维操作，这是个动态的过程。

![](https://github.com/reminisce/mxboard-demo/raw/master/pic/embedding_t_sne_motion.gif)

随着 t-SNE 算法的收敛，可以很明显地看到图片集在三维空间中被分成了几类。

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/imagenet_resnet_152_embedding.png)

最后我们来验证一下图片分类是否正确。在 TensorBoard GUI的右上角输入"dog"，所有打了"dog"标记的图片将被高亮。拖动并放大至高亮图片处，可以看到很多狗的图片，这表明预训练的 Resnet-152 模型是准确的。

![](https://github.com/reminisce/mxboard-demo/blob/master/pic/imagenet_resnet_152_dog_cluster.gif)

全部代码和具体说明请点[这里](https://github.com/reminisce/mxboard-demo#visualizing-convnet-codes-as-embeddings)。

## 后记

通过实战 MXBoard，我们可以看到，可视化工具在监督训练模型和解释深度学习原理方面的强大作用。MXBoard 给 MXNet 提供了一个在科研和生产环境中简单、易用、集中的可视化方案。当所有的代码在后台运行时，你需要的只是一个浏览器。

看了这么多，聪明的你是不是有很多精彩的可视化点子跃跃欲试呢？点击[这里](https://discuss.gluon.ai/)分享炫酷图片，让 MXNet 动起来。

特别感谢[郑子豪](https://github.com/zihaolucky)在项目开发时提供的技术支持。